{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linglong/data/linglong/.conda/envs/acc/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import literature\n",
    "\n",
    "R = sparse.load_npz(\"data/thrm_vertex_matrix.npz\")\n",
    "mats = np.array(open(\"data/thrm_mats.txt\", \"r\").read().splitlines())\n",
    "props = [\"thermoelectric\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrs = np.loadtxt('data/thrm_years.txt')\n",
    "R = R[(yrs>=1996)*(yrs<=2000),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = literature.hypergraph(R, mats, props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 20                 # length of the walk\n",
    "size = 1                    # number of the walk\n",
    "prop_ind = R.shape[1]-1     # column index of the property as the starting node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['thermoelectric CeGeNi a_886058 a_885358 Al10Ce10NiPd9 a_886058 a_885610 thermoelectric a_425231 a_524526 thermoelectric a_1710042 thermoelectric a_815535 KO5PTi K2O Na2O GeO2 a_99281 GeO2'],\n",
       " ['50739 50739 50737 50729 50729 52319 50739 9151 9151 9151 83553 83553 18658 18633 49949 19121 36848 646 646'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.random_walk(length, size, start_inds=prop_ind, rand_seed=0)    # uniform sampling\n",
    "\n",
    "# resulting in the following output: \n",
    "# (the first array is the sequence of selected nodes; the second array is the selected papers along the walk):\n",
    "# ---------------------\n",
    "# (['thermoelectric a_1244326 a_1084770 a_1085357 CoCrFeMnNi a_281555 a_1076970 CSi a_10764 Al2O3\n",
    "# K2O a_1672448 CaF2 a_460834 BaF2 a_638548 a_1287239 a_955446 a_955445 a_955447'],\n",
    "#  ['962469 1191497 746280 1191497 1421491 734403 1115449 132804 46832 1194889 1400463 1400463 23\n",
    "# 2314 232314 894012 1035899 1035899 615755 1075096'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rw_seqs.txt\", \"w\") as file:\n",
    "    for i in range(100):\n",
    "        rw_seqs = h.random_walk(length, size, start_inds=prop_ind, alpha=1, rand_seed=i)[0][0]    # non-uniform sampling (alpha=1)\n",
    "        file.write(rw_seqs+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5587"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils \n",
    "seqs = open(\"rw_seqs.txt\").read().splitlines()                              # reading the sequences\n",
    "seqs_noauthors = utils.remove_authors_from_RW(seqs)                         # removing the author nodes\n",
    "open(\"rw_seqs_noauthors.txt\", \"w\").write(\"\\n\".join(seqs_noauthors)+\"\\n\")    # saving the pruned sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 13:48:31,975 : INFO : Parsing lines (sentences) in: rw_seqs_noauthors.txt: \n",
      "2023-11-07 13:48:31,977 : INFO : Parameters for parsing phrases are as follows:\n",
      "2023-11-07 13:48:31,978 : INFO : \tdepth: 2\n",
      "2023-11-07 13:48:31,979 : INFO : \tphrase_min_count: 10\n",
      "2023-11-07 13:48:31,980 : INFO : \tphrase_threshold: 15\n",
      "2023-11-07 13:48:31,982 : INFO : collecting all words and their counts\n",
      "2023-11-07 13:48:31,983 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2023-11-07 13:48:31,987 : INFO : collected 833 word types from a corpus of 660 words (unigram + bigrams) and 90 sentences\n",
      "2023-11-07 13:48:31,987 : INFO : using 833 counts as vocab in Phrases<0 vocab, min_count=10, threshold=15, max_vocab_size=40000000>\n",
      "2023-11-07 13:48:31,988 : INFO : source_vocab length 833\n",
      "2023-11-07 13:48:31,994 : INFO : Phraser built with 0 phrasegrams\n",
      "0it [00:00, ?it/s]\n",
      "2023-11-07 13:48:32,000 : INFO : collecting all words and their counts\n",
      "2023-11-07 13:48:32,000 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2023-11-07 13:48:32,007 : INFO : collected 833 word types from a corpus of 660 words (unigram + bigrams) and 90 sentences\n",
      "2023-11-07 13:48:32,009 : INFO : using 833 counts as vocab in Phrases<0 vocab, min_count=10, threshold=15, max_vocab_size=40000000>\n",
      "2023-11-07 13:48:32,009 : INFO : source_vocab length 833\n",
      "2023-11-07 13:48:32,014 : INFO : Phraser built with 0 phrasegrams\n",
      "0it [00:00, ?it/s]\n",
      "2023-11-07 13:48:32,019 : INFO : collecting all words and their counts\n",
      "2023-11-07 13:48:32,020 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-11-07 13:48:32,024 : INFO : collected 333 word types from a corpus of 660 raw words and 90 sentences\n",
      "2023-11-07 13:48:32,025 : INFO : Loading a fresh vocabulary\n",
      "2023-11-07 13:48:32,025 : INFO : effective_min_count=5 retains 9 unique words (2% of original 333, drops 324)\n",
      "2023-11-07 13:48:32,026 : INFO : effective_min_count=5 leaves 194 word corpus (29% of original 660, drops 466)\n",
      "2023-11-07 13:48:32,027 : INFO : deleting the raw counts dictionary of 333 items\n",
      "2023-11-07 13:48:32,027 : INFO : sample=0.0001 downsamples 9 most-common words\n",
      "2023-11-07 13:48:32,028 : INFO : downsampling leaves estimated 4 word corpus (2.5% of prior 194)\n",
      "2023-11-07 13:48:32,029 : INFO : constructing a huffman tree from 9 words\n",
      "2023-11-07 13:48:32,030 : INFO : built huffman tree with maximum node depth 5\n",
      "2023-11-07 13:48:32,030 : INFO : estimated required memory for 9 words and 200 dimensions: 27900 bytes\n",
      "2023-11-07 13:48:32,031 : INFO : resetting layer weights\n",
      "2023-11-07 13:48:32,038 : INFO : training model with 20 workers on 9 vocabulary and 200 features, using sg=1 hs=1 sample=0.0001 negative=15 window=8\n",
      "2023-11-07 13:48:32,039 : INFO : training on a 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2023-11-07 13:48:32,040 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2023-11-07 13:48:32,042 : INFO : Training the model using the following parameters:\n",
      "2023-11-07 13:48:32,042 : INFO : \tphrase_min_count: 10\n",
      "2023-11-07 13:48:32,043 : INFO : \tsize: 200\n",
      "2023-11-07 13:48:32,044 : INFO : \twindow: 8\n",
      "2023-11-07 13:48:32,045 : INFO : \tmin_count: 5\n",
      "2023-11-07 13:48:32,045 : INFO : \tsg: True\n",
      "2023-11-07 13:48:32,046 : INFO : \ths: True\n",
      "2023-11-07 13:48:32,046 : INFO : \tworkers: 20\n",
      "2023-11-07 13:48:32,047 : INFO : \tnegative: 15\n",
      "2023-11-07 13:48:32,048 : INFO : \tstart_alpha: 0.001\n",
      "2023-11-07 13:48:32,048 : INFO : \tend_alpha: 0.0001\n",
      "2023-11-07 13:48:32,049 : INFO : \tsubsample: 0.0001\n",
      "2023-11-07 13:48:32,050 : INFO : \tbatch: 5000\n",
      "2023-11-07 13:48:32,050 : INFO : \tepochs: 5\n",
      "2023-11-07 13:48:32,053 : INFO : The model will be saved in None\n",
      "2023-11-07 13:48:32,053 : INFO : training model with 20 workers on 9 vocabulary and 200 features, using sg=1 hs=1 sample=0.0001 negative=15 window=8\n",
      "2023-11-07 13:48:32,081 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2023-11-07 13:48:32,082 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2023-11-07 13:48:32,083 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2023-11-07 13:48:32,084 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2023-11-07 13:48:32,085 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2023-11-07 13:48:32,085 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2023-11-07 13:48:32,086 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2023-11-07 13:48:32,087 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2023-11-07 13:48:32,087 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2023-11-07 13:48:32,088 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2023-11-07 13:48:32,089 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2023-11-07 13:48:32,090 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2023-11-07 13:48:32,090 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2023-11-07 13:48:32,091 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2023-11-07 13:48:32,091 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2023-11-07 13:48:32,092 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2023-11-07 13:48:32,092 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-11-07 13:48:32,093 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-11-07 13:48:32,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-11-07 13:48:32,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-11-07 13:48:32,094 : INFO : EPOCH - 1 : training on 660 raw words (3 effective words) took 0.0s, 119 effective words/s\n",
      "2023-11-07 13:48:32,095 : INFO : 1 Epoch(s) done. Loss: 0.0, LR: 0.001\n",
      "2023-11-07 13:48:32,141 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2023-11-07 13:48:32,142 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2023-11-07 13:48:32,143 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2023-11-07 13:48:32,143 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2023-11-07 13:48:32,144 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2023-11-07 13:48:32,144 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2023-11-07 13:48:32,145 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2023-11-07 13:48:32,146 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2023-11-07 13:48:32,147 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2023-11-07 13:48:32,147 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2023-11-07 13:48:32,148 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2023-11-07 13:48:32,148 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2023-11-07 13:48:32,149 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2023-11-07 13:48:32,149 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2023-11-07 13:48:32,150 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2023-11-07 13:48:32,150 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2023-11-07 13:48:32,151 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-11-07 13:48:32,151 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-11-07 13:48:32,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-11-07 13:48:32,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-11-07 13:48:32,153 : INFO : EPOCH - 2 : training on 660 raw words (3 effective words) took 0.0s, 126 effective words/s\n",
      "2023-11-07 13:48:32,154 : INFO : 2 Epoch(s) done. Loss: 0.0, LR: 0.001\n",
      "2023-11-07 13:48:32,198 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2023-11-07 13:48:32,199 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2023-11-07 13:48:32,200 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2023-11-07 13:48:32,200 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2023-11-07 13:48:32,201 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2023-11-07 13:48:32,201 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2023-11-07 13:48:32,202 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2023-11-07 13:48:32,204 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2023-11-07 13:48:32,205 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2023-11-07 13:48:32,205 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2023-11-07 13:48:32,206 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2023-11-07 13:48:32,206 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2023-11-07 13:48:32,207 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2023-11-07 13:48:32,208 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2023-11-07 13:48:32,208 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2023-11-07 13:48:32,209 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2023-11-07 13:48:32,209 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-11-07 13:48:32,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-11-07 13:48:32,210 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-11-07 13:48:32,211 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-11-07 13:48:32,211 : INFO : EPOCH - 3 : training on 660 raw words (2 effective words) took 0.0s, 89 effective words/s\n",
      "2023-11-07 13:48:32,212 : INFO : 3 Epoch(s) done. Loss: 0.0, LR: 0.001\n",
      "2023-11-07 13:48:32,242 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2023-11-07 13:48:32,243 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2023-11-07 13:48:32,244 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2023-11-07 13:48:32,244 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2023-11-07 13:48:32,245 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2023-11-07 13:48:32,246 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2023-11-07 13:48:32,246 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2023-11-07 13:48:32,247 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2023-11-07 13:48:32,247 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2023-11-07 13:48:32,248 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2023-11-07 13:48:32,248 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2023-11-07 13:48:32,249 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2023-11-07 13:48:32,249 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2023-11-07 13:48:32,250 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2023-11-07 13:48:32,250 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2023-11-07 13:48:32,251 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2023-11-07 13:48:32,251 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-11-07 13:48:32,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-11-07 13:48:32,253 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-11-07 13:48:32,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-11-07 13:48:32,254 : INFO : EPOCH - 4 : training on 660 raw words (5 effective words) took 0.0s, 208 effective words/s\n",
      "2023-11-07 13:48:32,254 : INFO : 4 Epoch(s) done. Loss: 0.0, LR: 0.001\n",
      "2023-11-07 13:48:32,285 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2023-11-07 13:48:32,286 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2023-11-07 13:48:32,287 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2023-11-07 13:48:32,288 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2023-11-07 13:48:32,288 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2023-11-07 13:48:32,289 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2023-11-07 13:48:32,290 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2023-11-07 13:48:32,290 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2023-11-07 13:48:32,292 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2023-11-07 13:48:32,293 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2023-11-07 13:48:32,294 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2023-11-07 13:48:32,295 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2023-11-07 13:48:32,296 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2023-11-07 13:48:32,297 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2023-11-07 13:48:32,298 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2023-11-07 13:48:32,299 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2023-11-07 13:48:32,299 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2023-11-07 13:48:32,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2023-11-07 13:48:32,300 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2023-11-07 13:48:32,301 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2023-11-07 13:48:32,301 : INFO : EPOCH - 5 : training on 660 raw words (9 effective words) took 0.0s, 376 effective words/s\n",
      "2023-11-07 13:48:32,302 : INFO : 5 Epoch(s) done. Loss: 26.10529327392578, LR: 0.001\n",
      "2023-11-07 13:48:32,305 : INFO : training on a 3300 raw words (22 effective words) took 0.3s, 88 effective words/s\n",
      "2023-11-07 13:48:32,306 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "seqs_noauthor_path = \"rw_seqs_noauthors.txt\"\n",
    "\n",
    "import embedding\n",
    "embed = embedding.dww2v(seqs_noauthor_path, workers=20)     # initiating deepwalk model with a different value for parameter workers\n",
    "embed.build_model()\n",
    "embed.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims,_,reordered_mats = embed.similarities(['thermoelectric'], mats, return_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_R = sparse.load_npz(\"data/thrm_vertex_matrix.npz\")\n",
    "subgraph_R = full_R[yrs<=2000]\n",
    "studied_mats = mats[np.asarray(np.sum(subgraph_R[:,h.nA:-1].multiply(subgraph_R[:,-1]), axis=0)>0)[0,:]]\n",
    "candidate_mats = mats[~np.isin(mats,studied_mats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sims,_,reordered_mats \u001b[39m=\u001b[39m embed\u001b[39m.\u001b[39msimilarities([\u001b[39m'\u001b[39m\u001b[39mthermoelectric\u001b[39m\u001b[39m'\u001b[39m], candidate_mats, return_nan\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# reporting 50 materials with highest likelihood of being thermoelectric\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m preds \u001b[39m=\u001b[39m reordered_mats[np\u001b[39m.\u001b[39margsort(\u001b[39m-\u001b[39msims[\u001b[39m0\u001b[39m,:])][:\u001b[39m50\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "sims,_,reordered_mats = embed.similarities(['thermoelectric'], candidate_mats, return_nan=False)\n",
    "\n",
    "# reporting 50 materials with highest likelihood of being thermoelectric\n",
    "preds = reordered_mats[np.argsort(-sims[0,:])][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m gt_discs \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdata/thrm_groundtruth_discs.json\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)) \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m yearwise_precs \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39misin(preds,gt_discs[\u001b[39mstr\u001b[39m(x)])\u001b[39m.\u001b[39msum()\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(preds) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2001\u001b[39m,\u001b[39m2019\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m np\u001b[39m.\u001b[39mcumsum(yearwise_precs)\n",
      "\u001b[1;32m/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m gt_discs \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdata/thrm_groundtruth_discs.json\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)) \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m yearwise_precs \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39misin(preds,gt_discs[\u001b[39mstr\u001b[39m(x)])\u001b[39m.\u001b[39msum()\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(preds) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2001\u001b[39m,\u001b[39m2019\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnhs/home/linglong/data2/linglong/accelerate-discoveries/test.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m np\u001b[39m.\u001b[39mcumsum(yearwise_precs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "gt_discs = json.load(open(\"data/thrm_groundtruth_discs.json\",\"r\")) \n",
    "yearwise_precs = [np.isin(preds,gt_discs[str(x)]).sum()/len(preds) for x in range(2001,2019)]\n",
    "np.cumsum(yearwise_precs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
